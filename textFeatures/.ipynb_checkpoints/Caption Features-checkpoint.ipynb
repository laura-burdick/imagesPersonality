{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate text features for captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import imp\n",
    "from operator import add\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change these variables to work with your setup\n",
    "home = 'data2016/'\n",
    "TEXT_FILE = home+\"text_features/allText.csv\" #all caption text (throughout, you might need to change the names of the different columns to match your file)\n",
    "STANFORD_NER_MODEL = '/Users/laura/software/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "STANFORD_NER_JAR = '/Users/laura/software/stanford-ner/stanford-ner.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures = pd.read_csv(TEXT_FILE)\n",
    "textFeatures = textFeatures.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger(STANFORD_NER_MODEL,\n",
    "               STANFORD_NER_JAR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize all comment file text\n",
    "textFeatures['commentfile1_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['commentfile1'].fillna('')]\n",
    "textFeatures['commentfile2_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['commentfile2'].fillna('')]\n",
    "textFeatures['commentfile3_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['commentfile3'].fillna('')]\n",
    "textFeatures['commentfile4_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['commentfile4'].fillna('')]\n",
    "textFeatures['commentfile5_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['commentfile5'].fillna('')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run NER on all tokenized comment file text\n",
    "print('1')\n",
    "textFeatures['commentfile1_NER'] = [st.tag(text) for text in textFeatures['commentfile1_tokenized']]\n",
    "print('2')\n",
    "textFeatures['commentfile2_NER'] = [st.tag(text) for text in textFeatures['commentfile2_tokenized']]\n",
    "print('3')\n",
    "textFeatures['commentfile3_NER'] = [st.tag(text) for text in textFeatures['commentfile3_tokenized']]\n",
    "print('4')\n",
    "textFeatures['commentfile4_NER'] = [st.tag(text) for text in textFeatures['commentfile4_tokenized']]\n",
    "print('5')\n",
    "textFeatures['commentfile5_NER'] = [st.tag(text) for text in textFeatures['commentfile5_tokenized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures.to_csv(TEXT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specifically, look for length of different NER categories in comment text\n",
    "textFeatures['commentfile1_NER_PERSON'] = [len([1 for (word,entity) in nerList if entity==\"PERSON\"]) for nerList in textFeatures['commentfile1_NER']]\n",
    "textFeatures['commentfile1_NER_ORG'] = [len([1 for (word,entity) in nerList if entity==\"ORGANIZATION\"]) for nerList in textFeatures['commentfile1_NER']]\n",
    "textFeatures['commentfile1_NER_LOCATION'] = [len([1 for (word,entity) in nerList if entity==\"LOCATION\"]) for nerList in textFeatures['commentfile1_NER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['commentfile2_NER_PERSON'] = [nerList.count(\", 'PERSON')\") for nerList in textFeatures['commentfile2_NER']]\n",
    "textFeatures['commentfile2_NER_ORG'] = [nerList.count(\", 'ORGANIZATION')\") for nerList in textFeatures['commentfile2_NER']]\n",
    "textFeatures['commentfile2_NER_LOCATION'] = [nerList.count(\", 'LOCATION')\") for nerList in textFeatures['commentfile2_NER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['commentfile3_NER_PERSON'] = [len([1 for (word,entity) in nerList if entity==\"PERSON\"]) for nerList in textFeatures['commentfile3_NER']]\n",
    "textFeatures['commentfile3_NER_ORG'] = [len([1 for (word,entity) in nerList if entity==\"ORGANIZATION\"]) for nerList in textFeatures['commentfile3_NER']]\n",
    "textFeatures['commentfile3_NER_LOCATION'] = [len([1 for (word,entity) in nerList if entity==\"LOCATION\"]) for nerList in textFeatures['commentfile3_NER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['commentfile4_NER_PERSON'] = [nerList.count(\", 'PERSON')\") for nerList in textFeatures['commentfile4_NER']]\n",
    "textFeatures['commentfile4_NER_ORG'] = [nerList.count(\", 'ORGANIZATION')\") for nerList in textFeatures['commentfile4_NER']]\n",
    "textFeatures['commentfile4_NER_LOCATION'] = [nerList.count(\", 'LOCATION')\") for nerList in textFeatures['commentfile4_NER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['commentfile5_NER_PERSON'] = [len([1 for (word,entity) in nerList if entity==\"PERSON\"]) for nerList in textFeatures['commentfile5_NER']]\n",
    "textFeatures['commentfile5_NER_ORG'] = [len([1 for (word,entity) in nerList if entity==\"ORGANIZATION\"]) for nerList in textFeatures['commentfile5_NER']]\n",
    "textFeatures['commentfile5_NER_LOCATION'] = [len([1 for (word,entity) in nerList if entity==\"LOCATION\"]) for nerList in textFeatures['commentfile5_NER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine all captions NER results\n",
    "textFeatures['all_captions_NER_PERSON'] = textFeatures['commentfile1_NER_PERSON'] + \\\n",
    "    textFeatures['commentfile2_NER_PERSON'] + textFeatures['commentfile3_NER_PERSON'] + \\\n",
    "    textFeatures['commentfile4_NER_PERSON'] + textFeatures['commentfile5_NER_PERSON']\n",
    "textFeatures['all_captions_NER_ORG'] = textFeatures['commentfile1_NER_ORG'] + textFeatures['commentfile2_NER_ORG'] + \\\n",
    "    textFeatures['commentfile3_NER_ORG'] + textFeatures['commentfile4_NER_ORG'] + textFeatures['commentfile5_NER_ORG']\n",
    "textFeatures['all_captions_NER_LOCATION'] = textFeatures['commentfile1_NER_LOCATION'] + \\\n",
    "    textFeatures['commentfile2_NER_LOCATION'] + textFeatures['commentfile3_NER_LOCATION'] + \\\n",
    "    textFeatures['commentfile4_NER_LOCATION'] + textFeatures['commentfile5_NER_LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for it,row in textFeatures.iterrows():\n",
    "    one = row.commentfile1\n",
    "    if one != one:\n",
    "        one = ''\n",
    "    two = row.commentfile2\n",
    "    if two != two:\n",
    "        two = ''\n",
    "    three = row.commentfile3\n",
    "    if three != three:\n",
    "        three = ''\n",
    "    four = row.commentfile4\n",
    "    if four != four:\n",
    "        four = ''\n",
    "    five = row.commentfile5\n",
    "    if five != five:\n",
    "        five = ''\n",
    "    all_captions.append(one + ' ' + two + ' ' + three + ' ' + four + ' ' + five)\n",
    "textFeatures['all_captions'] = all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwcLabels = [i for i in textFeatures['all_captions_liwc'][0][:textFeatures['all_captions_liwc'][0].rfind('\\n')+1].split(' ') \\\n",
    " if i[:4]==\"LIWC\" or i[:3]==\"MRC\"]\n",
    "len(liwcLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(liwcLabels)):\n",
    "    textFeatures['all_captions_'+liwcLabels[i]] = \\\n",
    "        [liwcArray[liwcArray.rfind('\\n')+1:].split(',')[i] for liwcArray in textFeatures['all_captions_liwc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['all_captions_num_words'] = [len([i for i in nltk.word_tokenize(all_captions) if i.isalpha()]) \\\n",
    "                                          for all_captions in textFeatures['all_captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['all_captions_long_words'] = [len([i for i in nltk.word_tokenize(all_captions) if len(i) > 6]) \\\n",
    "                                          for all_captions in textFeatures['all_captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['all_captions_num_numbers'] = [len([i for i in nltk.word_tokenize(all_captions) if i.isdigit()]) \\\n",
    "                                          for all_captions in textFeatures['all_captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp.reload(readability)\n",
    "stats = ['FRE','ARI','FK','CL','GFI','SMOG']\n",
    "for statI in range(len(stats)):\n",
    "    for i in range(1,6):\n",
    "        textFeatures['commentfile' + str(i) + '_' + stats[statI]] = [readability.compute_readability(caption)[statI] \\\n",
    "                                                    for caption in textFeatures['commentfile' + str(i)].fillna('')]\n",
    "    textFeatures['all_captions_' + stats[statI]] = textFeatures['commentfile1_' + stats[statI]] + \\\n",
    "        textFeatures['commentfile2_' + stats[statI]] + textFeatures['commentfile3_' + stats[statI]] + \\\n",
    "        textFeatures['commentfile4_' + stats[statI]] + textFeatures['commentfile5_' + stats[statI]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures.to_csv(TEXT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures[['commentfile1','commentfile2','commentfile3','commentfile4','commentfile5']] = \\\n",
    "    textFeatures[['commentfile1','commentfile2','commentfile3','commentfile4','commentfile5']].fillna('')\n",
    "textFeatures['all_captions'] = textFeatures['commentfile1'] + ' ' + textFeatures['commentfile2'] + ' ' + \\\n",
    "    textFeatures['commentfile3'] + ' ' + textFeatures['commentfile4'] + ' ' + textFeatures['commentfile5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unstemmed unigrams\n",
    "textFeatures['all_captions_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['all_captions']]\n",
    "\n",
    "#stemmed unigrams\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "textFeatures['all_captions_stemmed'] = [[lancaster.stem(token) for token in tokenizedList] for tokenizedList in \\\n",
    "                           textFeatures['all_captions_tokenized']]\n",
    "\n",
    "#stemmed bigrams\n",
    "ngrams_words = [ngrams(i,2) for i in textFeatures['all_captions_stemmed']]\n",
    "textFeatures['all_captions_bigrams'] = [[w for w in ngrams_word] for ngrams_word in ngrams_words]\n",
    "\n",
    "#stemmed trigrams\n",
    "ngrams_words = [ngrams(i,3) for i in textFeatures['all_captions_stemmed']]\n",
    "textFeatures['all_captions_trigrams'] = [[w for w in ngrams_word] for ngrams_word in ngrams_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate POS unigrams, bigrams, and trigrams\n",
    "tagged_text = [nltk.pos_tag(i) for i in textFeatures['all_captions_tokenized']]\n",
    "tagger_list = [[w[1] for w in text] for text in tagged_text]\n",
    "ngrams_words = [ngrams(t_list,2) for t_list in tagger_list]\n",
    "textFeatures['all_captions_pos_bigrams'] = [[w for w in ngrams_word] for ngrams_word in ngrams_words]\n",
    "\n",
    "ngrams_words = [ngrams(t_list,1) for t_list in tagger_list]\n",
    "textFeatures['all_captions_pos_unigrams'] = [[w for w in ngrams_word] for ngrams_word in ngrams_words]\n",
    "\n",
    "ngrams_words = [ngrams(t_list,3) for t_list in tagger_list]\n",
    "textFeatures['all_captions_pos_trigrams'] = [[w for w in ngrams_word] for ngrams_word in ngrams_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures.to_csv(TEXT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns a dataframe where the rows are the students and the columns are the vocabulary elements\n",
    "#An __oov vocabulary element is included at the end, for all out of vocabulary words\n",
    "#\n",
    "#Each element in the dataframe is the count of how many times that vocabulary element appears in the student's \n",
    "#writing\n",
    "def calculateVocabulary(column,minWordCount=5):\n",
    "    word_set = Counter()\n",
    "    for vocab_list in np.ravel(textFeatures[column]):\n",
    "        for word in vocab_list:\n",
    "            word_set[word] += 1\n",
    "\n",
    "    vocab = []\n",
    "    oov = []\n",
    "    for word,count in word_set.items():\n",
    "        if count >= minWordCount:\n",
    "            vocab.append(word)\n",
    "        else:\n",
    "            oov.append(word)\n",
    "\n",
    "    print(\"There are %d unique vocabulary elements and %d out of vocabulary elements for the %s column\" \\\n",
    "          % (len(vocab),len(oov),column))\n",
    "\n",
    "    df = pd.DataFrame(index = textFeatures.index, columns = vocab+['__oov'])\n",
    "    df = df.fillna(value=0)\n",
    "\n",
    "    for word in vocab:\n",
    "        df[word] = [i.count(word) for i in textFeatures[column]]\n",
    "    for word in oov:\n",
    "        df['__oov'] += [i.count(word) for i in textFeatures[column]]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = calculateVocabulary('all_captions_stemmed')\n",
    "unigrams['userid'] = textFeatures['id']\n",
    "unigrams.to_csv(home+\"text_features/all_captions_unigrams.csv\")\n",
    "\n",
    "bigrams = calculateVocabulary('all_captions_bigrams')\n",
    "bigrams['userid'] = textFeatures['id']\n",
    "bigrams.to_csv(home+\"text_features/all_captions_bigrams.csv\")\n",
    "\n",
    "trigrams = calculateVocabulary('all_captions_trigrams')\n",
    "trigrams['userid'] = textFeatures['id']\n",
    "trigrams.to_csv(home+\"text_features/all_captions_trigrams.csv\")\n",
    "\n",
    "pos_unigrams = calculateVocabulary('all_captions_pos_unigrams')\n",
    "pos_unigrams['userid'] = textFeatures['id']\n",
    "pos_unigrams.to_csv(home+\"text_features/all_captions_pos_unigrams.csv\")\n",
    "\n",
    "pos_bigrams = calculateVocabulary('all_captions_pos_bigrams')\n",
    "pos_bigrams['userid'] = textFeatures['id']\n",
    "pos_bigrams.to_csv(home+\"text_features/all_captions_pos_bigrams.csv\")\n",
    "\n",
    "pos_trigrams = calculateVocabulary('all_captions_pos_trigrams')\n",
    "pos_trigrams['userid'] = textFeatures['id']\n",
    "pos_trigrams.to_csv(home+\"text_features/all_captions_pos_trigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#char_unigrams\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1, 1), min_df=5)\n",
    "counts = ngram_vectorizer.fit_transform(textFeatures['all_captions'])\n",
    "\n",
    "char_unigrams = pd.DataFrame(data=counts.toarray().astype(int),index=textFeatures.index,\n",
    "                             columns=ngram_vectorizer.get_feature_names())\n",
    "char_unigrams['id'] = textFeatures['id']\n",
    "char_unigrams.to_csv(home+\"text_features/all_captions_char_unigrams.csv\")\n",
    "\n",
    "#char_bigrams\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_df=5)\n",
    "counts = ngram_vectorizer.fit_transform(textFeatures['all_captions'])\n",
    "\n",
    "char_bigrams = pd.DataFrame(data=counts.toarray().astype(int),index=textFeatures.index,\n",
    "                             columns=ngram_vectorizer.get_feature_names())\n",
    "char_bigrams['id'] = textFeatures['id']\n",
    "char_bigrams.to_csv(home+\"text_features/all_captions_char_bigrams.csv\")\n",
    "\n",
    "#char_trigrams\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=5)\n",
    "counts = ngram_vectorizer.fit_transform(textFeatures['all_captions'])\n",
    "\n",
    "char_trigrams = pd.DataFrame(data=counts.toarray().astype(int),index=textFeatures.index,\n",
    "                             columns=ngram_vectorizer.get_feature_names())\n",
    "char_trigrams['id'] = textFeatures['id']\n",
    "char_trigrams.to_csv(home+\"text_features/all_captions_char_trigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
