{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change these variables to work with your setup\n",
    "TEXT_FEATURES_FILE = \"data2015/text_features/allText.csv\" #text features\n",
    "AVERAGE_IMAGE_FEATURES_FILE =\"data2015/image_features/features_image_all_averages.csv\" #average image features\n",
    "IMAGE_FEATURES_FILE = \"data2015/image_features/features_image_all.csv\" #image features\n",
    "PRETRAINED_WORD2VEC = '/Users/laura/software/word2vec/GoogleNews-vectors-negative300.bin' #pre-trained word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures = pd.read_csv(TEXT_FEATURES_FILE)\n",
    "textFeatures = textFeatures.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "imageFeatures = pd.read_csv(AVERAGE_IMAGE_FEATURES_FILE)\n",
    "imageFeatures = imageFeatures.drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "individualImageFeatures = pd.read_csv(IMAGE_FEATURES_FILE)\n",
    "individualImageFeatures = individualImageFeatures.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MICRO IEUS (e.g., combined_image_unigrams)\n",
    "#First, combine all of the image attributes (colors, scenes, places, etc.)\n",
    "#Then, run the unigram extractor from these image attributes\n",
    "\n",
    "#MACRO IEUS (e.g., all_image_unigrams)\n",
    "#First, run the unigram extractor for each individual image (given those attributes)\n",
    "#Then, concatenate all of the unigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MICRO IEUS\n",
    "colorLabels = ['black_norm', 'blue_norm', 'brown_norm', 'grey_norm', 'green_norm', 'orange_norm', 'pink_norm', \n",
    "          'purple_norm', 'red_norm', 'white_norm','yellow_norm']\n",
    "colorNames = ['black','blue','brown','grey','green','orange','pink','purple','red','white','yellow']\n",
    "placeLabels = [i for i in individualImageFeatures.columns.values if i[:6] == \"PLACES\"]\n",
    "objLabels = [i for i in individualImageFeatures.columns.values if i[:8] == \"IMAGENET\"]\n",
    "\n",
    "all_bow = []\n",
    "\n",
    "for index,row in individualImageFeatures.iterrows():\n",
    "    bow = []\n",
    "    \n",
    "    for i in range(len(colorLabels)):\n",
    "        if row[colorLabels[i]] > 0.33:\n",
    "            bow.append(colorNames[i])\n",
    "            \n",
    "    #Find max place\n",
    "    maxValue = -1\n",
    "    maxPlace = \"\"\n",
    "    for place in placeLabels:\n",
    "        if maxValue < 0 or row[place] > maxValue:\n",
    "            maxValue = row[place]\n",
    "            maxPlace = place\n",
    "    if maxPlace == \"PLACES baseball_field\":\n",
    "        maxPlace = \"PLACES baseball field\"\n",
    "    bow += (maxPlace.split(' ')[1:])\n",
    "    \n",
    "    for obj in objLabels:\n",
    "        if row[obj] > 0:\n",
    "            bow += (obj[:-1].split(' ')[2:])\n",
    "            \n",
    "    all_bow.append(bow)\n",
    "    \n",
    "individualImageFeatures['combined_image_unigrams'] = all_bow\n",
    "individualImageFeatures.to_csv(IMAGE_FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACRO IEUS\n",
    "#Concatenate all of the individual image unigrams vectors into a single unigram vector for each person\n",
    "unigrams = {}\n",
    "\n",
    "for index,row in individualImageFeatures.iterrows():\n",
    "    _id = row['id']\n",
    "    image_unigrams = row['combined_image_unigrams']\n",
    "    \n",
    "    if _id in unigrams:\n",
    "        unigrams[_id] += image_unigrams\n",
    "    else:\n",
    "        unigrams[_id] = image_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_unigrams = []\n",
    "for _id in imageFeatures['id']:\n",
    "    all_image_unigrams.append(unigrams[_id])\n",
    "imageFeatures['all_image_unigrams'] = all_image_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above, but preserve which unigrams go with which images\n",
    "unigrams = {}\n",
    "\n",
    "for index,row in individualImageFeatures.iterrows():\n",
    "    _id = row['id']\n",
    "    imageNum = row['imageNum']\n",
    "    image_unigrams = row['combined_image_unigrams']\n",
    "    \n",
    "    unigrams[_id + '_' + str(imageNum)[0]] = image_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image1_unigrams = []\n",
    "image2_unigrams = []\n",
    "image3_unigrams = []\n",
    "image4_unigrams = []\n",
    "image5_unigrams = []\n",
    "for _id in imageFeatures['id']:\n",
    "    if _id + '_1' in unigrams:\n",
    "        image1_unigrams.append(unigrams[_id + '_1'])\n",
    "    else:\n",
    "        image1_unigrams.append([])\n",
    "        \n",
    "    if _id + '_2' in unigrams:\n",
    "        image2_unigrams.append(unigrams[_id + '_2'])\n",
    "    else:\n",
    "        image2_unigrams.append([])\n",
    "        \n",
    "    if _id + '_3' in unigrams:\n",
    "        image3_unigrams.append(unigrams[_id + '_3'])\n",
    "    else:\n",
    "        image3_unigrams.append([])\n",
    "        \n",
    "    if _id + '_4' in unigrams:\n",
    "        image4_unigrams.append(unigrams[_id + '_4'])\n",
    "    else:\n",
    "        image4_unigrams.append([])\n",
    "        \n",
    "    if _id + '_5' in unigrams:\n",
    "        image5_unigrams.append(unigrams[_id + '_5'])\n",
    "    else:\n",
    "        image5_unigrams.append([])\n",
    "\n",
    "imageFeatures['image1_unigrams'] = image1_unigrams\n",
    "imageFeatures['image2_unigrams'] = image2_unigrams\n",
    "imageFeatures['image3_unigrams'] = image3_unigrams\n",
    "imageFeatures['image4_unigrams'] = image4_unigrams\n",
    "imageFeatures['image5_unigrams'] = image5_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFeatures.to_csv(AVERAGE_IMAGE_FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge text features and image features\n",
    "textFeatures = textFeatures.merge(imageFeatures[['id','image1_unigrams','image2_unigrams', \\\n",
    "                                'image3_unigrams','image4_unigrams','image5_unigrams']],left_on='id',right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize unigrams\n",
    "textFeatures['all_captions_all_image_unigrams_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['all_captions']] + \\\n",
    "    textFeatures['all_image_unigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stem unigrams\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "textFeatures['all_captions_all_image_unigrams_stemmed'] = [[lancaster.stem(token) for token in tokenizedList] for \\\n",
    "                                            tokenizedList in textFeatures['all_captions_all_image_unigrams_tokenized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec model\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(PRETRAINED_WORD2VEC, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures['all_captions_tokenized'] = [nltk.word_tokenize(i) for i in textFeatures['all_captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word2vec - train on training data\n",
    "all_embeddings = []\n",
    "for tokenized_comment in textFeatures['all_captions_combined_image_unigrams_tokenized']:\n",
    "    full_embedding = [0] * 300\n",
    "    number = 0\n",
    "    for token in ast.literal_eval(tokenized_comment):\n",
    "        if token in model:\n",
    "            embedding = model[token]\n",
    "            number += 1\n",
    "            for i in range(len(full_embedding)):\n",
    "                full_embedding[i] += embedding[i]\n",
    "    if number > 0:\n",
    "        full_embedding = [i/number for i in full_embedding]\n",
    "    all_embeddings.append(full_embedding)\n",
    "textFeatures['all_captions_combined_image_unigrams_word2vec_averaged'] = all_embeddings\n",
    "\n",
    "for i in range(300):\n",
    "    textFeatures['all_captions_combined_image_unigrams_word2vec_averaged_' + str(i)] = \\\n",
    "        [embedding[i] for embedding in textFeatures['all_captions_combined_image_unigrams_word2vec_averaged']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFeatures.to_csv(TEXT_FEATURES_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
